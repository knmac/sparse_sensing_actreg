{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from src.utils.load_cfg import ConfigLoader\n",
    "from src.factories import ModelFactory\n",
    "from src.factories import DatasetFactory\n",
    "from src.utils.misc import MiscUtils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "model_cfg = '../configs/model_cfgs/pipeline5_rgbspec_san19pairfreeze_actreggru3_top1_cat.yaml'\n",
    "dataset_cfg = '../configs/dataset_cfgs/epickitchens_short.yaml'\n",
    "train_cfg = '../configs/train_cfgs/train_san_freeze_short.yaml'\n",
    "weight = '/home/knmac/Dropbox/SparseSensing/DGX_training_logs/' \\\n",
    "         'run_pipeline5_rgbspec_san19pairfreeze_actreggru3_top1_cat/best.model'\n",
    "\n",
    "# dataset_cfg = '../configs/dataset_cfgs/epickitchens.yaml'\n",
    "# train_cfg = '../configs/train_cfgs/train_san_freeze_adam_50.yaml'\n",
    "# weight = '/uploaded/run_pipeline5_rgbspec_san19pairfreeze_actreggru3_top1_cat/best.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "model_name, model_params = ConfigLoader.load_model_cfg(model_cfg)\n",
    "dataset_name, dataset_params = ConfigLoader.load_dataset_cfg(dataset_cfg)\n",
    "train_params = ConfigLoader.load_train_cfg(train_cfg)\n",
    "\n",
    "dataset_params.update({\n",
    "    'modality': model_params['modality'],\n",
    "    'num_segments': model_params['num_segments'],\n",
    "    'new_length': model_params['new_length'],\n",
    "})\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Build model\n",
    "model_factory = ModelFactory()\n",
    "model = model_factory.generate(model_name, device=device, model_factory=model_factory, **model_params)\n",
    "model.load_model(weight)\n",
    "model = model.to(device)\n",
    "\n",
    "# Get training augmentation and transforms\n",
    "train_augmentation = MiscUtils.get_train_augmentation(model.modality, model.crop_size)\n",
    "train_transform, val_transform = MiscUtils.get_train_val_transforms(\n",
    "    modality=model.modality,\n",
    "    input_mean=model.input_mean,\n",
    "    input_std=model.input_std,\n",
    "    scale_size=model.scale_size,\n",
    "    crop_size=model.crop_size,\n",
    "    train_augmentation=train_augmentation,\n",
    ")\n",
    "\n",
    "# Data loader\n",
    "dataset_factory = DatasetFactory()\n",
    "loader_params = {\n",
    "    'batch_size': train_params['batch_size'],\n",
    "    'num_workers': train_params['num_workers'],\n",
    "    'pin_memory': True,\n",
    "}\n",
    "\n",
    "val_dataset = dataset_factory.generate(dataset_name, mode='val', transform=val_transform, **dataset_params)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, **loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_wrapper_pipeline5(model, x):\n",
    "    _rgb_high = x['RGB']\n",
    "    _rgb_low = model._downsample(_rgb_high)\n",
    "    _spec = x['Spec']\n",
    "    batch_size = _rgb_high.shape[0]\n",
    "\n",
    "    # Extract low resolutions features ------------------------------------\n",
    "    assert model.low_feat_model.modality == ['RGB', 'Spec']\n",
    "    low_feat, spec_feat = model.low_feat_model({'RGB': _rgb_low, 'Spec': _spec},\n",
    "                                              return_concat=False)\n",
    "\n",
    "    # (B*T, C) --> (B, T, C)\n",
    "    low_feat = low_feat.view([batch_size,\n",
    "                              model.num_segments,\n",
    "                              model.low_feat_model.feature_dim])\n",
    "    spec_feat = spec_feat.view([batch_size,\n",
    "                                model.num_segments,\n",
    "                                model.low_feat_model.feature_dim])\n",
    "\n",
    "    # Retrieve attention --------------------------------------------------\n",
    "    attn = model.low_feat_model.rgb.get_attention_weight(\n",
    "        l_name=model.attention_layer[0],\n",
    "        m_name=model.attention_layer[1],\n",
    "        aggregated=True,\n",
    "    )\n",
    "    attn = attn.view([-1, model.num_segments] + list(attn.shape[1:]))\n",
    "    model._attn = attn\n",
    "\n",
    "    # Spatial sampler -----------------------------------------------------\n",
    "    # Compute bboxes -> (B, T, top_k, 4)\n",
    "    bboxes = model.spatial_sampler.sample_multiple_frames(\n",
    "        attn, _rgb_high.shape[-1], reorder=True, avg_across_time=True)\n",
    "\n",
    "    # (B, T*C, H, W) -> (B, T, C, H, W)\n",
    "    _rgb_high = _rgb_high.view((-1, model.num_segments, 3) + _rgb_high.size()[-2:])\n",
    "    # model._check(_rgb_high, attn, bboxes)\n",
    "\n",
    "    # Extract regions and feed in high_feat_model\n",
    "    high_feat = []\n",
    "    for k in range(model.spatial_sampler.top_k):\n",
    "        high_feat_k = []\n",
    "        for b in range(batch_size):\n",
    "            tops = bboxes[b, :, k, 0]\n",
    "            lefts = bboxes[b, :, k, 1]\n",
    "            bottoms = bboxes[b, :, k, 2]\n",
    "            rights = bboxes[b, :, k, 3]\n",
    "\n",
    "            # Batch regions across time b/c of consisting size\n",
    "            regions_k_b = []\n",
    "            for t in range(model.num_segments):\n",
    "                regions_k_b.append(\n",
    "                    _rgb_high[b, t, :,\n",
    "                              tops[t]:bottoms[t],\n",
    "                              lefts[t]:rights[t]\n",
    "                              ].unsqueeze(dim=0))\n",
    "            regions_k_b = torch.cat(regions_k_b, dim=0)\n",
    "\n",
    "            # Tensor manipulation to prepare\n",
    "            regions_k_b = regions_k_b.unsqueeze(dim=0)\n",
    "            regions_k_b = regions_k_b.view(\n",
    "                [1, regions_k_b.shape[1]*regions_k_b.shape[2],\n",
    "                 regions_k_b.shape[3], regions_k_b.shape[4]])\n",
    "\n",
    "            # Feed the regions to high_feat_model\n",
    "            out = model.high_feat_model({'RGB': regions_k_b})\n",
    "            high_feat_k.append(out.unsqueeze(dim=0))\n",
    "\n",
    "        # Concat across batch dim and collect\n",
    "        high_feat_k = torch.cat(high_feat_k, dim=0)\n",
    "        high_feat.append(high_feat_k)\n",
    "\n",
    "    assert len(high_feat) == model.spatial_sampler.top_k\n",
    "    assert high_feat[0].shape[0] == batch_size\n",
    "    assert high_feat[0].shape[1] == model.num_segments\n",
    "\n",
    "    # Action recognition --------------------------------------------------\n",
    "    all_feats = torch.cat([low_feat, spec_feat] + high_feat, dim=2)\n",
    "\n",
    "    assert all_feats.ndim == 3\n",
    "    return all_feats\n",
    "\n",
    "\n",
    "def forward_wrapper_actreggru3(model, x, hidden=None):\n",
    "    if hidden is None:\n",
    "        hidden_global, hidden_local, hidden_both = None, None, None\n",
    "    else:\n",
    "        hidden_global, hidden_local, hidden_both = hidden\n",
    "\n",
    "    # Process concatenated input\n",
    "    x_global = x[..., :model.dim_global]\n",
    "    x_local = x[..., model.dim_global:]\n",
    "    assert x_local.shape[-1] == model.dim_local\n",
    "\n",
    "    # Multi head RNNs\n",
    "    if isinstance(model.num_class, (list, tuple)):\n",
    "        output = [0.0, 0.0]\n",
    "    else:\n",
    "        output = 0.0\n",
    "\n",
    "    if model.weight_global > 0:\n",
    "        output_global, hidden_global = model.actreg_global(x_global, hidden_global)\n",
    "        output = model.combine_output(output, output_global, model.weight_global)\n",
    "\n",
    "    if model.weight_local > 0:\n",
    "        output_local, hidden_local = model.actreg_local(x_local, hidden_local)\n",
    "        output = model.combine_output(output, output_local, model.weight_local)\n",
    "\n",
    "    if model.weight_both > 0:\n",
    "        output_both, hidden_both = model.actreg_both(x, hidden_both)\n",
    "        output = model.combine_output(output, output_both, model.weight_both)\n",
    "\n",
    "    hidden = [hidden_global, hidden_local, hidden_both]\n",
    "    return tuple(output), hidden, output_global, output_local, output_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.metrics import AverageMeter, accuracy, multitask_accuracy\n",
    "\n",
    "class MyMetrics():\n",
    "    def __init__(self):\n",
    "        self.losses = AverageMeter()\n",
    "        self.verb_losses = AverageMeter()\n",
    "        self.noun_losses = AverageMeter()\n",
    "        \n",
    "        self.top1 = AverageMeter()\n",
    "        self.top5 = AverageMeter()\n",
    "        \n",
    "        self.verb_top1 = AverageMeter()\n",
    "        self.verb_top5 = AverageMeter()\n",
    "        \n",
    "        self.noun_top1 = AverageMeter()\n",
    "        self.noun_top5 = AverageMeter()\n",
    "        \n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def update(self, target, output):\n",
    "        verb_output = output[0]\n",
    "        noun_output = output[1]\n",
    "        batch_size = verb_output.size(0)        \n",
    "            \n",
    "        loss_verb = self.criterion(verb_output, target['verb'])\n",
    "        loss_noun = self.criterion(noun_output, target['noun'])\n",
    "        loss = (loss_verb + loss_noun)/2\n",
    "\n",
    "        self.losses.update(loss.item(), batch_size)\n",
    "        self.verb_losses.update(loss_verb.item(), batch_size)\n",
    "        self.noun_losses.update(loss_noun.item(), batch_size)\n",
    "        \n",
    "        verb_prec1, verb_prec5 = accuracy(verb_output, target['verb'], topk=(1, 5))\n",
    "        self.verb_top1.update(verb_prec1, batch_size)\n",
    "        self.verb_top5.update(verb_prec5, batch_size)\n",
    "\n",
    "        noun_prec1, noun_prec5 = accuracy(noun_output, target['noun'], topk=(1, 5))\n",
    "        self.noun_top1.update(noun_prec1, batch_size)\n",
    "        self.noun_top5.update(noun_prec5, batch_size)\n",
    "        \n",
    "        prec1, prec5 = multitask_accuracy((verb_output, noun_output),\n",
    "                                          (target['verb'], target['noun']),\n",
    "                                          topk=(1, 5))\n",
    "        self.top1.update(prec1, batch_size)\n",
    "        self.top5.update(prec5, batch_size)\n",
    "    \n",
    "    def print_out(self):\n",
    "        print('Loss = {:.04f}'.format(self.losses.avg))\n",
    "        print('Top1 = {:.04f}'.format(self.top1.avg))\n",
    "        print('Top5 = {:.04f}'.format(self.top5.avg))\n",
    "        \n",
    "        print('Verb Loss = {:.04f}'.format(self.verb_losses.avg))\n",
    "        print('Verb Top1 = {:.04f}'.format(self.verb_top1.avg))\n",
    "        print('Verb Top5 = {:.04f}'.format(self.verb_top5.avg))\n",
    "        \n",
    "        print('Noun Loss = {:.04f}'.format(self.noun_losses.avg))\n",
    "        print('Noun Top1 = {:.04f}'.format(self.noun_top1.avg))\n",
    "        print('Noun Top5 = {:.04f}'.format(self.noun_top5.avg))\n",
    "        \n",
    "    def collect_acc(self):\n",
    "        return [\n",
    "            self.top1.avg, self.top5.avg,\n",
    "            self.verb_top1.avg, self.verb_top5.avg,\n",
    "            self.noun_top1.avg, self.noun_top5.avg\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_global = model.actreg_model.weight_global\n",
    "weight_local = model.actreg_model.weight_local\n",
    "weight_both = model.actreg_model.weight_both\n",
    "\n",
    "global_metrics, local_metrics, both_metrics, fuse_metrics = MyMetrics(), MyMetrics(), MyMetrics(), MyMetrics()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (sample, target) in enumerate(val_loader):\n",
    "        sample = {k: v.to(device) for k, v in sample.items()}\n",
    "        target = {k: v.to(device) for k, v in target.items()}\n",
    "\n",
    "        # real_out = model(sample)\n",
    "        all_feats = forward_wrapper_pipeline5(model, sample)\n",
    "        res = forward_wrapper_actreggru3(model.actreg_model, all_feats)\n",
    "        _, _, output_global, output_local, output_both = res\n",
    "        output_fuse = (\n",
    "            output_global[0]*weight_global + output_local[0]*weight_local + output_both[0]*weight_both,\n",
    "            output_global[1]*weight_global + output_local[1]*weight_local + output_both[1]*weight_both,\n",
    "        )\n",
    "        \n",
    "        global_metrics.update(target, output_global)\n",
    "        local_metrics.update(target, output_local)\n",
    "        both_metrics.update(target, output_both)\n",
    "        fuse_metrics.update(target, output_fuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Analyze multi-head\n",
      "| Head |Top1|Top5 |Verb Top1|Verb Top5|Noun Top1|Noun Top5|\n",
      "|------|---:|----:|--------:|--------:|--------:|--------:|\n",
      "|Global|   0| 0.00|     0.00|    4.142|     0.00|    2.663|\n",
      "|Local |   0| 0.00|     0.00|    0.000|     0.00|    0.000|\n",
      "|Both  |  25|52.22|    56.21|   85.059|    34.91|   56.805|\n",
      "|Fuse  |  25|52.22|    56.07|   85.207|    34.76|   56.953|\n"
     ]
    }
   ],
   "source": [
    "from pytablewriter import MarkdownTableWriter\n",
    "\n",
    "writer = MarkdownTableWriter(\n",
    "    table_name=\"Analyze multi-head\",\n",
    "    headers=[\"Head\", \"Top1\", \"Top5\", \"Verb Top1\", \"Verb Top5\", \"Noun Top1\", \"Noun Top5\"],\n",
    "    value_matrix=[\n",
    "        [\"Global\"] + global_metrics.collect_acc(),\n",
    "        [\"Local\"] + local_metrics.collect_acc(),\n",
    "        [\"Both\"] + both_metrics.collect_acc(),\n",
    "        [\"Fuse\"] + fuse_metrics.collect_acc(),\n",
    "    ],\n",
    ")\n",
    "writer.write_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
