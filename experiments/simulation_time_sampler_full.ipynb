{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple simulation for time sampling\"\"\"\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from src.factories import ModelFactory\n",
    "from src.factories import DatasetFactory\n",
    "from src.utils.load_cfg import ConfigLoader\n",
    "from src.utils.misc import MiscUtils\n",
    "from src.models.pytorch_ssim.ssim import SSIM\n",
    "\n",
    "from tools.complexity import (get_model_complexity_info,\n",
    "                              is_supported_instance,\n",
    "                              flops_to_string,\n",
    "                              get_model_parameters_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and analyze FLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_flops(model, units='GMac', precision=3):\n",
    "    \"\"\"Wrapper to collect flops and number of parameters at each layer\"\"\"\n",
    "    total_flops = model.compute_average_flops_cost()\n",
    "\n",
    "    def accumulate_flops(self):\n",
    "        if is_supported_instance(self):\n",
    "            return self.__flops__ / model.__batch_counter__\n",
    "        else:\n",
    "            sum = 0\n",
    "            for m in self.children():\n",
    "                sum += m.accumulate_flops()\n",
    "            return sum\n",
    "\n",
    "    def flops_repr(self):\n",
    "        accumulated_flops_cost = self.accumulate_flops()\n",
    "        return ', '.join([flops_to_string(accumulated_flops_cost, units=units, precision=precision),\n",
    "                          '{:.3%} MACs'.format(accumulated_flops_cost / total_flops),\n",
    "                          self.original_extra_repr()])\n",
    "\n",
    "    def add_extra_repr(m):\n",
    "        m.accumulate_flops = accumulate_flops.__get__(m)\n",
    "        flops_extra_repr = flops_repr.__get__(m)\n",
    "        if m.extra_repr != flops_extra_repr:\n",
    "            m.original_extra_repr = m.extra_repr\n",
    "            m.extra_repr = flops_extra_repr\n",
    "            assert m.extra_repr != m.original_extra_repr\n",
    "\n",
    "    def del_extra_repr(m):\n",
    "        if hasattr(m, 'original_extra_repr'):\n",
    "            m.extra_repr = m.original_extra_repr\n",
    "            del m.original_extra_repr\n",
    "        if hasattr(m, 'accumulate_flops'):\n",
    "            del m.accumulate_flops\n",
    "\n",
    "    model.apply(add_extra_repr)\n",
    "\n",
    "    # Retrieve flops and param at each layer and sub layer (2 levels)\n",
    "    flops_dict, param_dict = {}, {}\n",
    "    for i in model._modules.keys():\n",
    "        item = model._modules[i]\n",
    "        if isinstance(model._modules[i], torch.nn.modules.container.Sequential):\n",
    "            for j in model._modules[i]._modules.keys():\n",
    "                key = '{}-{}'.format(i, j)\n",
    "                flops_dict[key] = item._modules[j].accumulate_flops()\n",
    "                param_dict[key] = get_model_parameters_number(item._modules[j])\n",
    "        else:\n",
    "            flops_dict[i] = item.accumulate_flops()\n",
    "            param_dict[i] = get_model_parameters_number(item)\n",
    "\n",
    "    model.apply(del_extra_repr)\n",
    "    return flops_dict, param_dict\n",
    "\n",
    "\n",
    "def analyze_flops_all_models(model, device):\n",
    "    # Compute FLOPS for different models\n",
    "    model_factory = ModelFactory()\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # RGB model\n",
    "    print('RGB model')\n",
    "    rgb_model = model.light_model.rgb\n",
    "\n",
    "    macs, params = get_model_complexity_info(\n",
    "        rgb_model,\n",
    "        (3, 224, 244),\n",
    "        as_strings=True,\n",
    "        print_per_layer_stat=False,\n",
    "    )\n",
    "\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "    print('\\n{:<15} {:>12} {:>12} {:>12} {:>12}\\n'.format(\n",
    "        'Layer', 'Flops (GMac)', 'Param (M)', 'AccFlops', 'AccParam') + '-'*67)\n",
    "\n",
    "    flops_dict, param_dict = collect_flops(rgb_model)\n",
    "    total_flops, total_param = 0, 0\n",
    "    flops_rgb_part1 = 0\n",
    "    for k in flops_dict:\n",
    "        total_flops += flops_dict[k]*1e-9\n",
    "        total_param += param_dict[k]*1e-6\n",
    "        if k == 'layer3-0':\n",
    "            flops_rgb_part1 = total_flops * 1e9\n",
    "        print('{:<15} {:>12.5f} {:>12.5f} {:>12.2f} {:>12.2f}'.format(\n",
    "            k, flops_dict[k]*1e-9, param_dict[k]*1e-6, total_flops, total_param))\n",
    "    del rgb_model\n",
    "    flops_rgb_part2 = total_flops*1e9 - flops_rgb_part1\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # Spec model\n",
    "    print('\\n\\nSpec model')\n",
    "    spec_model = model.light_model.spec\n",
    "\n",
    "    macs, params = get_model_complexity_info(\n",
    "        spec_model,\n",
    "        (1, 224, 244),\n",
    "        as_strings=True,\n",
    "        print_per_layer_stat=False,\n",
    "    )\n",
    "\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "    print('\\n{:<15} {:>12} {:>12} {:>12} {:>12}\\n'.format(\n",
    "        'Layer', 'Flops (GMac)', 'Param (M)', 'AccFlops', 'AccParam') + '-'*67)\n",
    "\n",
    "    flops_dict, param_dict = collect_flops(spec_model)\n",
    "    total_flops, total_param = 0, 0\n",
    "    flops_spec_part1 = 0\n",
    "    for k in flops_dict:\n",
    "        total_flops += flops_dict[k]*1e-9\n",
    "        total_param += param_dict[k]*1e-6\n",
    "        if k == 'layer3-0':\n",
    "            flops_spec_part1 = total_flops * 1e9\n",
    "        print('{:<15} {:>12.5f} {:>12.5f} {:>12.2f} {:>12.2f}'.format(\n",
    "            k, flops_dict[k]*1e-9, param_dict[k]*1e-6, total_flops, total_param))\n",
    "    del spec_model\n",
    "    flops_spec_part2 = total_flops*1e9 - flops_spec_part1\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # Hallucination model\n",
    "    # create dummy model because the one in real model checks for num_segment\n",
    "    # here we only look at complexity per frame\n",
    "    print('\\n\\nHallu model')\n",
    "    hallu_model = model_factory.generate(\n",
    "        model_name='HalluConvLSTM',\n",
    "        device=device,\n",
    "        num_segments=1,  # Test per frame\n",
    "        attention_dim=[32, 14, 14],\n",
    "        rnn_input_dim=32,\n",
    "        rnn_hidden_dim=32,\n",
    "        rnn_num_layers=1,\n",
    "        has_encoder_decoder=True,\n",
    "    ).to(device)\n",
    "    macs, params = get_model_complexity_info(\n",
    "        hallu_model,\n",
    "        (1, 32, 14, 14),  # Test per frame\n",
    "        as_strings=True,\n",
    "        print_per_layer_stat=False,\n",
    "    )\n",
    "\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "    print('\\n{:<15} {:>12} {:>12} {:>12} {:>12}\\n'.format(\n",
    "        'Layer', 'Flops (GMac)', 'Param (M)', 'AccFlops', 'AccParam') + '-'*67)\n",
    "\n",
    "    flops_dict, param_dict = collect_flops(hallu_model)\n",
    "    total_flops, total_param = 0, 0\n",
    "    for k in flops_dict:\n",
    "        total_flops += flops_dict[k]*1e-9\n",
    "        total_param += param_dict[k]*1e-6\n",
    "        print('{:<15} {:>12.5f} {:>12.5f} {:>12.2f} {:>12.2f}'.format(\n",
    "            k, flops_dict[k]*1e-9, param_dict[k]*1e-6, total_flops, total_param))\n",
    "    del hallu_model\n",
    "    flops_hallu = total_flops*1e9\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # Action recognition model\n",
    "    # create dummy model because the one in real model checks for num_segment\n",
    "    # here we only look at complexity per frame\n",
    "    print('\\n\\nActreg model')\n",
    "    actreg_model = model_factory.generate(\n",
    "        model_name='ActregGRU',\n",
    "        device=device,\n",
    "        modality=['RGB', 'Spec'],\n",
    "        num_segments=1,  # Test per frame\n",
    "        num_class=[125, 352],\n",
    "        dropout=0.5,\n",
    "        feature_dim=2048,\n",
    "        rnn_input_size=512,\n",
    "        rnn_hidden_size=512,\n",
    "        rnn_num_layers=1,\n",
    "    ).to(device)\n",
    "\n",
    "    macs, params = get_model_complexity_info(\n",
    "        actreg_model,\n",
    "        (1, 2*2048),  # Test 2 modalities\n",
    "        as_strings=True,\n",
    "        print_per_layer_stat=False,\n",
    "    )\n",
    "\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "    print('\\n{:<15} {:>12} {:>12} {:>12} {:>12}\\n'.format(\n",
    "        'Layer', 'Flops', 'Param', 'AccFlops', 'AccParam') + '-'*67)\n",
    "\n",
    "    flops_dict, param_dict = collect_flops(actreg_model)\n",
    "    total_flops, total_param = 0, 0\n",
    "    for k in flops_dict:\n",
    "        total_flops += flops_dict[k]\n",
    "        total_param += param_dict[k]\n",
    "        print('{:<15} {:>12.0f} {:>12.0f} {:>12.0f} {:>12.0f}'.format(\n",
    "            k, flops_dict[k], param_dict[k], total_flops, total_param))\n",
    "    del actreg_model\n",
    "    flops_actreg = total_flops\n",
    "\n",
    "    return {'rgb_part1': flops_rgb_part1,\n",
    "            'rgb_part2': flops_rgb_part2,\n",
    "            'spec_part1': flops_spec_part1,\n",
    "            'spec_part2': flops_spec_part2,\n",
    "            'hallu': flops_hallu,\n",
    "            'actreg': flops_actreg,\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All frame action recognition with and without sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actreg_nosample(model, sample):\n",
    "    # Feed to the feature extraction\n",
    "    x = model.light_model(sample)\n",
    "\n",
    "    # Feed to the actreg model with appropriate number of frames\n",
    "    model.actreg_model.rnn.flatten_parameters()\n",
    "    x = model.actreg_model.relu(model.actreg_model.fc1(x))\n",
    "    x = x.view(-1, model.num_segments, model.actreg_model.rnn_input_size)\n",
    "    x, _ = model.actreg_model.rnn(x, None)\n",
    "    x = model.actreg_model.relu(x)\n",
    "\n",
    "    output_list_nosampled = []\n",
    "    for t in range(model.num_segments):\n",
    "        output_list_nosampled.append(model.actreg_model.classify(x[:, t, :]))\n",
    "\n",
    "    score_verb_nosampled = np.stack([F.softmax(item[0], dim=1)[0].cpu()\n",
    "                                     for item in output_list_nosampled], axis=0)\n",
    "    score_noun_nosampled = np.stack([F.softmax(item[1], dim=1)[0].cpu()\n",
    "                                     for item in output_list_nosampled], axis=0)\n",
    "    return score_verb_nosampled, score_noun_nosampled\n",
    "\n",
    "\n",
    "def actreg_withsample(model, sample, ssim_list, theta):\n",
    "    # Feed to the feature extraction\n",
    "    x = model.light_model(sample)\n",
    "\n",
    "    # Sample to ignore frames with low SSIM\n",
    "    x = x[ssim_list > theta]\n",
    "    num_sampled = x.shape[0]\n",
    "\n",
    "    # Feed to the actreg model with appropriate number of frames\n",
    "    model.actreg_model.rnn.flatten_parameters()\n",
    "    x = model.actreg_model.relu(model.actreg_model.fc1(x))\n",
    "    x = x.view(-1, num_sampled, model.actreg_model.rnn_input_size)\n",
    "    x, _ = model.actreg_model.rnn(x, None)\n",
    "    x = model.actreg_model.relu(x)\n",
    "\n",
    "    output_list_sampled = []\n",
    "    t1, t2 = 0, 0\n",
    "    while t1 < model.num_segments:\n",
    "        if ssim_list[t1] > theta:  # not removed frame -> compute new result\n",
    "            output_list_sampled.append(model.actreg_model.classify(x[:, t2, :]))\n",
    "            t2 += 1\n",
    "        else:  # removed frame -> reuse prev result\n",
    "            output_list_sampled.append(output_list_sampled[-1])\n",
    "        t1 += 1\n",
    "    score_verb_sampled = np.stack([F.softmax(item[0], dim=1)[0].cpu()\n",
    "                                   for item in output_list_sampled], axis=0)\n",
    "    score_noun_sampled = np.stack([F.softmax(item[1], dim=1)[0].cpu()\n",
    "                                   for item in output_list_sampled], axis=0)\n",
    "    return score_verb_sampled, score_noun_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB model\n",
      "Computational complexity:       4.08 GMac\n",
      "Number of parameters:           15.55 M \n",
      "\n",
      "Layer           Flops (GMac)    Param (M)     AccFlops     AccParam\n",
      "-------------------------------------------------------------------\n",
      "conv_in              0.01049      0.00019         0.01         0.00\n",
      "bn_in                0.00700      0.00013         0.02         0.00\n",
      "conv0                0.05597      0.00410         0.07         0.00\n",
      "bn0                  0.00175      0.00013         0.08         0.00\n",
      "layer0-0             0.05012      0.00287         0.13         0.01\n",
      "layer0-1             0.05012      0.00287         0.18         0.01\n",
      "layer0-2             0.05012      0.00287         0.23         0.01\n",
      "conv1                0.05597      0.01638         0.28         0.03\n",
      "bn1                  0.00175      0.00051         0.28         0.03\n",
      "layer1-0             0.24619      0.04245         0.53         0.07\n",
      "layer1-1             0.24619      0.04245         0.78         0.11\n",
      "layer1-2             0.24619      0.04245         1.02         0.16\n",
      "conv2                0.11010      0.13107         1.13         0.29\n",
      "bn2                  0.00086      0.00102         1.13         0.29\n",
      "layer2-0             0.22117      0.16758         1.35         0.46\n",
      "layer2-1             0.22117      0.16758         1.58         0.62\n",
      "layer2-2             0.22117      0.16758         1.80         0.79\n",
      "layer2-3             0.22117      0.16758         2.02         0.96\n",
      "conv3                0.11010      0.52429         2.13         1.48\n",
      "bn3                  0.00043      0.00205         2.13         1.49\n",
      "layer3-0             0.21092      0.66590         2.34         2.15\n",
      "layer3-1             0.21092      0.66590         2.55         2.82\n",
      "layer3-2             0.21092      0.66590         2.76         3.48\n",
      "layer3-3             0.21092      0.66590         2.97         4.15\n",
      "layer3-4             0.21092      0.66590         3.18         4.82\n",
      "layer3-5             0.21092      0.66590         3.39         5.48\n",
      "conv4                0.10276      2.09715         3.50         7.58\n",
      "bn4                  0.00020      0.00410         3.50         7.58\n",
      "layer4-0             0.19214      2.65479         3.69        10.24\n",
      "layer4-1             0.19214      2.65479         3.88        12.89\n",
      "layer4-2             0.19214      2.65479         4.07        15.55\n",
      "relu                 0.00599      0.00000         4.08        15.55\n",
      "pool                 0.00589      0.00000         4.08        15.55\n",
      "avgpool              0.00010      0.00000         4.08        15.55\n",
      "\n",
      "\n",
      "Spec model\n",
      "Computational complexity:       4.08 GMac\n",
      "Number of parameters:           15.55 M \n",
      "\n",
      "Layer           Flops (GMac)    Param (M)     AccFlops     AccParam\n",
      "-------------------------------------------------------------------\n",
      "conv_in              0.00350      0.00006         0.00         0.00\n",
      "bn_in                0.00700      0.00013         0.01         0.00\n",
      "conv0                0.05597      0.00410         0.07         0.00\n",
      "bn0                  0.00175      0.00013         0.07         0.00\n",
      "layer0-0             0.05012      0.00287         0.12         0.01\n",
      "layer0-1             0.05012      0.00287         0.17         0.01\n",
      "layer0-2             0.05012      0.00287         0.22         0.01\n",
      "conv1                0.05597      0.01638         0.27         0.03\n",
      "bn1                  0.00175      0.00051         0.28         0.03\n",
      "layer1-0             0.24619      0.04245         0.52         0.07\n",
      "layer1-1             0.24619      0.04245         0.77         0.11\n",
      "layer1-2             0.24619      0.04245         1.01         0.16\n",
      "conv2                0.11010      0.13107         1.12         0.29\n",
      "bn2                  0.00086      0.00102         1.13         0.29\n",
      "layer2-0             0.22117      0.16758         1.35         0.46\n",
      "layer2-1             0.22117      0.16758         1.57         0.62\n",
      "layer2-2             0.22117      0.16758         1.79         0.79\n",
      "layer2-3             0.22117      0.16758         2.01         0.96\n",
      "conv3                0.11010      0.52429         2.12         1.48\n",
      "bn3                  0.00043      0.00205         2.12         1.49\n",
      "layer3-0             0.21092      0.66590         2.33         2.15\n",
      "layer3-1             0.21092      0.66590         2.54         2.82\n",
      "layer3-2             0.21092      0.66590         2.75         3.48\n",
      "layer3-3             0.21092      0.66590         2.96         4.15\n",
      "layer3-4             0.21092      0.66590         3.18         4.82\n",
      "layer3-5             0.21092      0.66590         3.39         5.48\n",
      "conv4                0.10276      2.09715         3.49         7.58\n",
      "bn4                  0.00020      0.00410         3.49         7.58\n",
      "layer4-0             0.19214      2.65479         3.68        10.24\n",
      "layer4-1             0.19214      2.65479         3.87        12.89\n",
      "layer4-2             0.19214      2.65479         4.07        15.55\n",
      "relu                 0.00599      0.00000         4.07        15.55\n",
      "pool                 0.00589      0.00000         4.08        15.55\n",
      "avgpool              0.00010      0.00000         4.08        15.55\n",
      "\n",
      "\n",
      "Hallu model\n",
      "Computational complexity:       0.02 GMac\n",
      "Number of parameters:           92.48 k \n",
      "\n",
      "Layer           Flops (GMac)    Param (M)     AccFlops     AccParam\n",
      "-------------------------------------------------------------------\n",
      "relu                 0.00002      0.00000         0.00         0.00\n",
      "rnn                  0.01448      0.07386         0.01         0.07\n",
      "conv_encoder         0.00181      0.00925         0.02         0.08\n",
      "bn_encoder           0.00001      0.00006         0.02         0.08\n",
      "conv_decoder         0.00181      0.00925         0.02         0.09\n",
      "bn_decoder           0.00001      0.00006         0.02         0.09\n",
      "\n",
      "\n",
      "Actreg model\n",
      "Computational complexity:       0.0 GMac\n",
      "Number of parameters:           3.92 M  \n",
      "\n",
      "Layer                  Flops        Param     AccFlops     AccParam\n",
      "-------------------------------------------------------------------\n",
      "relu                    1024            0         1024            0\n",
      "dropout_layer              0            0         1024            0\n",
      "fc1                        1      2097664         1025      2097664\n",
      "rnn                  1579520      1575936      1580545      3673600\n",
      "fc_verb                64000        64125      1644545      3737725\n",
      "fc_noun               180224       180576      1824769      3918301\n"
     ]
    }
   ],
   "source": [
    "# dataset_cfg = '../configs/dataset_cfgs/epickitchens_short.yaml'\n",
    "dataset_cfg = '../configs/dataset_cfgs/epickitchens.yaml'\n",
    "train_cfg = '../configs/train_cfgs/train_san_freeze_short.yaml'\n",
    "model_cfg = '../configs/model_cfgs/pipeline2_rgbspec_san19pairfreeze_actreggru_halluconvlstm.yaml'\n",
    "theta = -0.25\n",
    "\n",
    "# weight = 'saved_models/san19freeze_halluconvlstm_actreggru/dim32_layer1_nsegment3/epoch_00049.model'\n",
    "# model_cfg_mod = None\n",
    "weight = '../saved_models/san19freeze_halluconvlstm_actreggru/dim32_layer1_nsegment10/epoch_00049.model'\n",
    "model_cfg_mod = {'num_segments': 10, 'hallu_model_cfg': 'exp_cfgs/haluconvlstm_32_1.yaml'}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Build model and data loader\n",
    "# Load configurations\n",
    "model_name, model_params = ConfigLoader.load_model_cfg(model_cfg)\n",
    "dataset_name, dataset_params = ConfigLoader.load_dataset_cfg(dataset_cfg)\n",
    "train_params = ConfigLoader.load_train_cfg(train_cfg)\n",
    "\n",
    "if model_cfg_mod is not None:\n",
    "    model_params.update(model_cfg_mod)\n",
    "\n",
    "dataset_params.update({\n",
    "    'modality': model_params['modality'],\n",
    "    'num_segments': model_params['num_segments'],\n",
    "    'new_length': model_params['new_length'],\n",
    "})\n",
    "\n",
    "# Build model\n",
    "model_factory = ModelFactory()\n",
    "model = model_factory.generate(model_name, device=device,\n",
    "                               model_factory=model_factory, **model_params)\n",
    "model.load_model(weight)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Get training augmentation and transforms\n",
    "train_augmentation = MiscUtils.get_train_augmentation(model.modality, model.crop_size)\n",
    "train_transform, val_transform = MiscUtils.get_train_val_transforms(\n",
    "    modality=model.modality,\n",
    "    input_mean=model.input_mean,\n",
    "    input_std=model.input_std,\n",
    "    scale_size=model.scale_size,\n",
    "    crop_size=model.crop_size,\n",
    "    train_augmentation=train_augmentation,\n",
    ")\n",
    "\n",
    "# Data loader\n",
    "dataset_factory = DatasetFactory()\n",
    "loader_params = {\n",
    "    'batch_size': train_params['batch_size'],\n",
    "    'num_workers': train_params['num_workers'],\n",
    "    'pin_memory': True,\n",
    "}\n",
    "\n",
    "val_dataset = dataset_factory.generate(dataset_name, mode='val',\n",
    "                                       transform=val_transform,\n",
    "                                       **dataset_params)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, **loader_params)\n",
    "\n",
    "# Analyze flops\n",
    "flops_dict = analyze_flops_all_models(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2398/2398 [22:15<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without sampling\n",
      "- Verb: 46.7348%\n",
      "- Noun: 30.0083%\n",
      "Accuracy with sampling\n",
      "- Verb: 39.3036%\n",
      "- Noun: 25.2335%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Go through the dataset\n",
    "ssim_criterion = SSIM(window_size=3, channel=32)\n",
    "correct_verb_nosampled = []\n",
    "correct_noun_nosampled = []\n",
    "correct_verb_sampled = []\n",
    "correct_noun_sampled = []\n",
    "ssim_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for i, (sample, target) in enumerate(val_loader):\n",
    "    for (sample, target) in tqdm(val_loader):\n",
    "        sample = {k: v.to(device) for k, v in sample.items()}\n",
    "        target = {k: v.to(device) for k, v in target.items()}\n",
    "        target_verb, target_noun = target['verb'].item(), target['noun'].item()\n",
    "\n",
    "        # Get attention and hallucination\n",
    "        model(sample)\n",
    "        attn = model._attn[0]\n",
    "        hallu = model._hallu[0]\n",
    "\n",
    "        # Compute ssim\n",
    "        ssim_list = np.zeros(model.num_segments)\n",
    "        for t in range(1, model.num_segments):\n",
    "            ssim = -ssim_criterion(attn[t].unsqueeze(dim=0),\n",
    "                                   hallu[t-1].unsqueeze(dim=0)).item()\n",
    "            ssim_list[t] = ssim\n",
    "\n",
    "        # Compute score for each frame\n",
    "        score_verb_nosampled, score_noun_nosampled = actreg_nosample(model, sample)\n",
    "        score_verb_sampled, score_noun_sampled = actreg_withsample(model, sample, ssim_list, theta)\n",
    "\n",
    "        # Collect results\n",
    "        ssim_all.append(ssim_list)\n",
    "        correct_verb_nosampled.append(score_verb_nosampled.argmax(axis=1) == target_verb)\n",
    "        correct_noun_nosampled.append(score_noun_nosampled.argmax(axis=1) == target_noun)\n",
    "        correct_verb_sampled.append(score_verb_sampled.argmax(axis=1) == target_verb)\n",
    "        correct_noun_sampled.append(score_noun_sampled.argmax(axis=1) == target_noun)\n",
    "\n",
    "        # if i % 100 == 0:\n",
    "        #     print('{}/{}'.format(i, len(val_loader)))\n",
    "\n",
    "# Print statistics\n",
    "print('Accuracy without sampling')\n",
    "print('- Verb: {:.4f}%'.format(np.concatenate(correct_verb_nosampled).mean() * 100))\n",
    "print('- Noun: {:.4f}%'.format(np.concatenate(correct_noun_nosampled).mean() * 100))\n",
    "print('Accuracy with sampling')\n",
    "print('- Verb: {:.4f}%'.format(np.concatenate(correct_verb_sampled).mean() * 100))\n",
    "print('- Noun: {:.4f}%'.format(np.concatenate(correct_noun_sampled).mean() * 100))\n",
    "\n",
    "ssim_all = np.concatenate(ssim_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set statistics:\n",
      "- Total FLOPS:               196224.01*1e9\n",
      "- Overall saved FLOPS:       99154.03*1e9\n",
      "- Overall saving percentage: 50.53%\n",
      "Per frame statistics (on average):\n",
      "- Original FLOPS per frame:  8.18*1e9\n",
      "- Saved FLOPS per frame:     4.13*1e9\n"
     ]
    }
   ],
   "source": [
    "n_removed = (ssim_all <= theta).sum()\n",
    "total_flops = len(ssim_all) * (flops_dict['rgb_part1'] + flops_dict['rgb_part2'] +\n",
    "                               flops_dict['spec_part1'] + flops_dict['spec_part2'] +\n",
    "                               flops_dict['hallu'] + flops_dict['actreg']\n",
    "                               )\n",
    "# sampled_flops = n_removed * (flops_dict['rgb_part1'] + flops_dict['hallu'])\n",
    "saved_flops = n_removed * (flops_dict['rgb_part2'] + \n",
    "                           flops_dict['spec_part1'] + flops_dict['spec_part2'] +\n",
    "                           flops_dict['actreg'])\n",
    "\n",
    "print('Val set statistics:')\n",
    "print('- Total FLOPS:               {:.2f}*1e9'.format(total_flops * 1e-9))\n",
    "print('- Overall saved FLOPS:       {:.2f}*1e9'.format(saved_flops * 1e-9))\n",
    "print('- Overall saving percentage: {:.2f}%'.format(saved_flops / total_flops * 100))\n",
    "print('Per frame statistics (on average):')\n",
    "print('- Original FLOPS per frame:  {:.2f}*1e9'.format(total_flops / len(ssim_all) * 1e-9))\n",
    "print('- Saved FLOPS per frame:     {:.2f}*1e9'.format(saved_flops / len(ssim_all) * 1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len frames = 23980\n",
      "saved frames = 17020\n",
      "saved frames percentage = 70.97581317764804\n",
      "----------\n",
      "full_com per frame  = 8182819269.000002\n",
      "saved_com per frame = 5825736345.000002\n",
      "----------\n",
      "full_com total  = 196224006070620.03\n",
      "saved_com total = 99154032591900.03\n",
      "saved_com total percent = 50.53104081272043\n"
     ]
    }
   ],
   "source": [
    "# Double check the math\n",
    "print('len frames =', len(ssim_all))\n",
    "print('saved frames =', (ssim_all <= theta).sum())\n",
    "print('saved frames percentage =', (ssim_all <= theta).sum() / len(ssim_all) * 100)\n",
    "\n",
    "print('-'*10)\n",
    "full_com = flops_dict['rgb_part1']  + flops_dict['rgb_part2'] + \\\n",
    "           flops_dict['spec_part1'] + flops_dict['spec_part2'] + \\\n",
    "           flops_dict['hallu']      + flops_dict['actreg']\n",
    "saved_com = flops_dict['rgb_part2'] + flops_dict['spec_part1'] + flops_dict['spec_part2'] + flops_dict['actreg']\n",
    "print('full_com per frame  =', full_com)\n",
    "print('saved_com per frame =', saved_com)\n",
    "\n",
    "print('-'*10)\n",
    "full_com_total  = full_com  * len(ssim_all)\n",
    "saved_com_total = saved_com * (ssim_all <= theta).sum()\n",
    "print('full_com total  =', full_com_total)\n",
    "print('saved_com total =', saved_com_total)\n",
    "print('saved_com total percent =', saved_com_total / full_com_total * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
