batch_size: 256
num_workers: 128
n_epochs: 50
lr_steps: [20, 30, 40]  # Epochs to decay learning rate by 10
print_freq: 20  # Print frequency
clip_gradient: 20.0  # Gradient norm clipping. Default = None
partialbn: True
freeze: True  # freeze all weights except fusion
eval_freq: 1
optimizer: 'Adam'
optim_params:
    lr: 0.01
